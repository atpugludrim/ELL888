\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\citation{MCM}
\citation{keropt}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{slt}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Statistical Learning Theory}{2}{subsection.2.1}\protected@file@percent }
\citation{slt}
\citation{MCM}
\citation{MCM}
\newlabel{eqn:pac}{{3}{3}{Statistical Learning Theory}{equation.2.3}{}}
\newlabel{eq:vareps}{{4}{3}{Statistical Learning Theory}{equation.2.4}{}}
\newlabel{eqn:vcbound}{{5}{3}{Statistical Learning Theory}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Minimal Complexity Machines~\cite  {MCM}}{3}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Motivation}{3}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}MCM formulations}{3}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Data points $\in \mathbb  {R}^1$\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{4}{Data points $\in \mathbb {R}^1$\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The red arrows are operations on margin and blue arrows on radius\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:2}{{2}{4}{The red arrows are operations on margin and blue arrows on radius\relax }{figure.caption.3}{}}
\newlabel{eq:1}{{23}{5}{MCM formulations}{equation.2.23}{}}
\newlabel{eq:2}{{24}{5}{MCM formulations}{equation.2.24}{}}
\newlabel{eq:3}{{25}{5}{MCM formulations}{equation.2.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Generalizing the MCM}{6}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{eq:mcmstart}{{30}{6}{Generalizing the MCM}{equation.2.30}{}}
\newlabel{eq:upbound}{{31}{6}{Generalizing the MCM}{equation.2.31}{}}
\newlabel{eq:mcmend}{{33}{6}{Generalizing the MCM}{equation.2.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Separating hyperplanes learned by MCM and SVM\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:mcmvsvm}{{3}{7}{Separating hyperplanes learned by MCM and SVM\relax }{figure.caption.4}{}}
\citation{amari}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Kernel Optimization}{8}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Motivation}{8}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Identity map, to compare with\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:3}{{4}{8}{Identity map, to compare with\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Formulations}{8}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A non-linear map that is steeper between the classes, shallower within (data dependent)\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:4}{{5}{9}{A non-linear map that is steeper between the classes, shallower within (data dependent)\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This improves class separability, and also compresses the data reducing VC-dimension\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:5}{{6}{9}{This improves class separability, and also compresses the data reducing VC-dimension\relax }{figure.caption.7}{}}
\citation{amari}
\citation{xiong}
\citation{xiong}
\citation{xiong}
\newlabel{eq:alk}{{34}{10}{Formulations}{equation.2.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces This picture is taken from Xiong {\em  et al.}~\cite  {xiong}. It shows how a kernel can harm class separability. (a) Input. (b) Two dimensional projection of empirical feature space for second order polynomial kernel. (c) Two dimensional projection of Gaussian kernel.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:6}{{7}{11}{This picture is taken from Xiong {\em et al.}~\cite {xiong}. It shows how a kernel can harm class separability. (a) Input. (b) Two dimensional projection of empirical feature space for second order polynomial kernel. (c) Two dimensional projection of Gaussian kernel.\relax }{figure.caption.8}{}}
\citation{xiong}
\citation{keropt}
\citation{keropt}
\newlabel{eq:B}{{38}{12}{Formulations}{equation.2.38}{}}
\newlabel{eq:W}{{39}{12}{Formulations}{equation.2.39}{}}
\newlabel{eq:gep}{{40}{12}{Formulations}{equation.2.40}{}}
\newlabel{eq:GEP}{{41}{12}{Formulations}{equation.2.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Kernel optimization using conformal maps for the minimal complexity machine~\cite  {keropt}}{12}{section.3}\protected@file@percent }
\citation{keropt}
\citation{keropt}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Training Data\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:data}{{8}{14}{Training Data\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training Data mapped through kernels and then projected to two dimensional Euclidean space through kernel PCA.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{fig:kernelPCA}{{9}{14}{Training Data mapped through kernels and then projected to two dimensional Euclidean space through kernel PCA.\relax }{figure.caption.10}{}}
\bibstyle{plain}
\bibcite{amari}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Training Data\relax }}{15}{figure.caption.11}\protected@file@percent }
\newlabel{fig:data}{{10}{15}{Training Data\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Training Data mapped through kernels and then projected to two dimensional Euclidean space through kernel PCA.\relax }}{15}{figure.caption.12}\protected@file@percent }
\newlabel{fig:kernelPCA}{{11}{15}{Training Data mapped through kernels and then projected to two dimensional Euclidean space through kernel PCA.\relax }{figure.caption.12}{}}
\bibcite{slt}{2}
\bibcite{MCM}{3}
\bibcite{xiong}{4}
\bibcite{keropt}{5}
\gdef \@abspage@last{16}
