#+TITLE: ELL888 Minor
#+AUTHOR: Mridul Gupta
#+DATE: Monday 21 February 2022 IST
#+OPTIONS: toc:nil
* Q1
This problem has a straight forward approach. First generate the
missing data and then use the data matrix as if it was a normal matrix
to learn graph structure.\par The data generation process itself,
however, is non-trivial. And the exact method used depends a lot on
the data being used as well as the mechanics of data being
missing. Since in this case the data is missing at random, one can
completely ignore the missing data and just use complete
features~\cite{links_1}.\par
Note that this only works when the fraction of data missing is not
huge. For example if \(70-80\%\) of data is missing, then we cannot
ignore all the data. However, in such cases the best remedy is to
collect more data properly.\par
Now, imputing new data can be done in several ways. One method is to
just replace the missing data points with a default value. A little
improvement over this is calculating the mean (or median) for each
feature and replacing missing value in a feature with it its mean.\par
The mean uses the distribution information, and we can go one step
further by learning the distribution/density on the features and
sample random variables from the distribution. This works for iid
samples and iid features.\par
This requires assumption of the form of distribution of the
features. One popular missing data generating method is the
EM-algorithm. The recursive EM-algorithm is more popularly used with
the assumption that the density is a mixture of Gaussians.\par
Another method can be to use can be to learn \(d\) regression models,
each for a feature. The \(k^{\text{th}}\) model uses features
\(\{i\}_{i\neq k}\) to predict the value of feature \(k\). This method
will inherently incorporate dependencies between features as well as
the learn the density of the feature we are predicting. One problem
might be some other feature \(j\neq k\) might also have missing value;
a solution to this is to use mean (or median) as the placeholder of
feature \(j\) during prediction of feature \(k\).
* Q2
/* Use MNIST */
\(y_i\in\{0,1\}\).
Choose a similarity score \(s(x_1,x_2)\). Update similarity score to
be \(\tilde{s}(x_1,x_2)=s(x_1,x_2)+k(1\{y_1=y_2\}-1\{y_1=1-y_2\})\) if
both \(y_1\text{ and }y_2\) are available, else just use \(s(x_1,
x_2)\) without the label information. Calculate second order
statistics matrix and learn graph.
* Q3
In this problem we have the data matrix \(X\in\mathbb{R}^{n\times d}\)
where both \(n\) and \(d\) are large, typically with \(n\gg d\). The
problem here is in calculating the second order statistics matrix. The
computation cost will be of the order \(O(n^2d)\). Since we need a
final adjacency and/or a weight matrix \(\in\mathbb{R}^{n\times n}\)
we cannot avoid the \(n^2\) in the cost term, but we can try to reduce
the \(d\) component.\par
Another problem is of fitting the big data matrix in memory to perform
computations. For really large matrices, they need to be stored in
hard drive which makes the computation difficult, unless one has
access to powerful compute resources.
** Developing the idea
One of the key things that makes what follows work is the assumption
(which would generally be true) that the data is quite sparse. This
means we can compress the information to a lower dimensional
space.\par
Now, there are several ways to go about this. One way could be to do
PCA (or SVD) on the data \(X\in\mathbb{R}^{n\times d}\) to get
\(X^{PCA}\in\mathbb{R}^{n\times k}\) for some \(\mathbb{Z}^+\ni k \ll
d\). This new data can then be used to learn the graph. The
computation complexity of PCA is \(O(\min(d^3,n^3)\) which is
\(O(d^3)\) in our case. And with the second order statistics
calculation the complexity becomes \(O(d^3+n^2k)\).\par
Another problem remains however: the closed form PCA algorithm makes use of the
full data matrix, which might not fit into the memory for really large
\(n\text{ and }d\).
** Better solution (?)
As mentioned in the hint, it would be good if we could use stochastic
gradient descent based approaches. And the straight forward solution
is to use the extension of PCA that makes use of stochastic gradient
descent: a deep learning architecture called \textbf{AutoEncoder}.\par
An AutoEncoder consists of two neural networks: an \textit{encoder}
and a \textit{decoder}.
[[/home/mridul/Desktop/exam/ELL888/autoencoder.png]]
The encoder takes in the data item \(x_i\in\mathbb{R}^d\) and outputs
an encoded embedding \(e_i\in\mathbb{R}^k\) while the decoder takes
this embedding and tries to reproduce the data from this compressed
representation. This can then be trained end-to-end using stochastic gradient
descent in minibatched fashion with mean squared error loss calculated
on \(x\text{ and }x'\). The idea is that if the embedding is useful
and retains information, then original data should be reproducible
given the embeddings.\par
Once the training is completed, the decoder is discarded and just the
encoder is used to generate embeddings from data. And once we have
this, we can learn the weight matrix using a kernel that looks at
pairs of data points at a time. For example a Gaussian Kernel can be
used as shown in equation~\ref{eqn:gaus}.
\begin{equation}
w_{ij}=\operatorname{exp}\biggl[-\frac{d(e_i,e_j)}{2\sigma^2}\biggr]
\label{eqn:gaus}
\end{equation}
Note: an autoencoder without non-linearities learns the same
embeddings as PCA.
** Advantages
- Easily parallelizable.
- Reduces data dimension that mitigates ``curse of dimensionality''.
- Works on part of data that can be loaded into memory from hard drive
  in advance by efficient data loader making it faster after the first
  few data loads.
- Can be trained on part of the data. It is generally required to have
  more data than the number of parameters to avoid overfitting. Using
  a validation set to keep a check on the generalization error
  provides further improvement.
** Disadvantages and challenges
- Fine tuning the parameter \(k\) is a challenge.
- Since this works like a look up (that is take a data point and
  generate its embedding), this can only be used with methods that
  generate \(w_{ij}\) by looking only at \(x_i\) and
  \(x_j\). Algorithms that use something like k-nearest neighbor
  information are still a challenge.
** Experimentation
I use the MNIST handwritten digit dataset~\cite{MNIST} for this
task. This consists of $60000$ images, each of size $28\times 28$, so
the data matrix \(\mathbf{X}\in\mathbb{R}^{60000\times 28\times 28}\)
or equivalently \(\mathbf{X}\in\mathbb{R}^{60000\times 784}\) after
converting the image matrices into vectors. This data is still small,
but it'll work for this purpose.\par
I choose two projection size \(15\text{ and }5\) and test on
both. Note this is a lot of compression and it is expected a lot of
information will be lost.\par
The \(60000\) sized dataset is still too large for my machine, and the
naive but unavoidable \(\displaystyle\frac{n(n-1)}{2}\) calculations take a lot of
time so I chose a still smaller subset of size \(3000\) to do my
weight matrix calculations. The only way to go beyond this is to
parallelize these calculations using GPU.
#+NAME: Projection size 15
[[/home/mridul/Desktop/exam/ELL888/sAutoencoder_training_curves.png]]
#+NAME: Projection size 5
[[/home/mridul/Desktop/exam/ELL888/ssAutoencoder_training_curves.png]]
* Q4
The problem with hetergeneous data can be solved by embedding the data
points into a real vector using something similar to the previous
question. Another much simpler approach is to simply define a distance
function that works with heterogeneous data to generate the second
order statistics matrix.\par
** Approach
The assumption here is that unlike the previous question, the data is
not of high volume, but that the challenge is that graph learning
frameworks usually work with numerical data.\par
So, we need a function that can calculate the elements of the second
order statistics matrix. And this can be done by defining a distance
function for various different kinds of data. Say we partion
\(\mathbf{X}\in\mathbb{R}^{n\times d_r}\times \mathbb{I}^{n\times d_i}
\times\mathbb{C}^{n\times d_c}\) as
\([\mathbf{X_R},\mathbf{X_I},\mathbf{X_C}]\). Then we can define a
suitable distance function on each of the partition.\par
For example, say we use the Euclidean distance (equation~\ref{eqn:eucl}) on both real and
integer partitions.
\begin{equation}
D_{\text{eucl}}(x^i,x^j) = \biggl(\sum_{k=1}^{d_{\text{partition}}}(x^i_k-x^j_k)^2\biggr)^{\frac{1}{2}}
\label{eqn:eucl}
\end{equation}
And use the Hamming distance after one-hot encoding the categorical
data (equation~\ref{eqn:hamm}).
\begin{equation}
D_{\text{hamm}}(x^i,x^j)=\sum_{k=1}^{d_c} x^i\oplus x^j
\label{eqn:hamm}
\end{equation}
With these two, the final distance can then be defined as a weighted
sum as shown in equation~\ref{eqn:dist}.
\begin{align*}
N(x^i, x^j)=&W_r\cdot D_{\text{eucl}}(x^i_{1:d_r},x^j_{1:d_r})\\
&+W_i\cdot _{\text{eucl}}(x^i_{d_r:d_r+d_i},x^j_{d_r:d_r+d_i})\\
&+W_c\cdot D_{\text{hamm}}(x^i_{d_r+d_i:d_r+d_i+d_c},x^j_{d_r+d_i:d_r+d_i+d_c})\\
\end{align*}
\begin{equation}
D(x^i,x^j)=\frac{N(x^i,x^j)}{W_r+W_i+W_c}
\label{eqn:dist}
\end{equation}
** Next steps
Once we have the second order matrix weights can be generated from it
using kernel methods, linear embeddings, nearest neighbor methods,
laplacian based methods for structural constraints, etc.
** Advantages
- Works with heterogeneous data
- Little modification required in anything else
** Disadvantages
- Works when data is small. If data is large however, then one must
  consider embedding to lower dimension similar to previous question
  since autoencoders can also be used with categorical (after one-hot
  encoding) and integer data.
** Other possible approaches
One can also consider using other data structures such as trees (and
metrics they use, like gini index or mutual information) since they
work well with heterogeneous data.
